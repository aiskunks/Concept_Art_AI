{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_and_write_info_to_file(url):\n",
    "    # Set a user-agent header to mimic a web browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Make the GET request with the headers\n",
    "    source = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    # Generate a file name based on the URL\n",
    "    file_name = url.split('/')[-1] + '.txt'\n",
    "\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        # Part 1: Extract and write title and lead paragraph\n",
    "        title_element = soup.find('h1')\n",
    "        title_text_parts = [part.strip() for part in title_element.text.split('\\n')]\n",
    "        title_text = ' '.join(title_text_parts)\n",
    "        file.write(title_text + '\\n')\n",
    "\n",
    "        lead_paragraph = soup.find('p', class_='lead')\n",
    "        lead_text = lead_paragraph.text.strip()\n",
    "        file.write(lead_text + '\\n\\n')\n",
    "\n",
    "        # Part 2: Extract and write content from sections with class \"heading\"\n",
    "        sections = soup.find_all('div', class_='heading')\n",
    "\n",
    "        for section in sections:\n",
    "            heading = section.h2.text.strip()\n",
    "            file.write(heading + '\\n')\n",
    "\n",
    "            content = section.find_next_siblings(['p', 'ul'])\n",
    "\n",
    "            for element in content:\n",
    "                if element.name == 'p':\n",
    "                    file.write(element.text.strip() + '\\n')\n",
    "                elif element.name == 'ul':\n",
    "                    for li in element.find_all('li'):\n",
    "                        file.write(li.text.strip() + '\\n')\n",
    "\n",
    "            file.write('\\n')\n",
    "        \n",
    "         # Part 3: Handle \"Application Steps\" section\n",
    "        application_steps_section = soup.find('section', class_='entry-content stepped-content')\n",
    "        if application_steps_section:\n",
    "            application_steps_title = application_steps_section.h2.text.strip()\n",
    "            file.write(application_steps_title + '\\n')\n",
    "            \n",
    "            for item in application_steps_section.find_all(['p', 'ul', 'ol']):\n",
    "                text = item.get_text(strip=True)\n",
    "                if text and text[0].isdigit():\n",
    "                    text = f\"{text[0]}.{text[1:]}\"\n",
    "                file.write(text + '\\n')\n",
    "\n",
    "        # Part 4: Extract and write links to the file\n",
    "        file.write(\"Links:\\n\")\n",
    "        # Find the \"Application Steps\" section by inspecting the HTML structure and class\n",
    "        application_steps_section = soup.find('div', class_='modal-body')\n",
    "\n",
    "        # Create a dictionary to store subheadings and their associated content\n",
    "        subheading_content = {}\n",
    "\n",
    "        if application_steps_section:\n",
    "            # Find all the h4 elements within the section\n",
    "            step_headers = application_steps_section.find_all('h4')\n",
    "\n",
    "            for step in step_headers:\n",
    "                step_title = step.text.strip()\n",
    "\n",
    "                # Find the associated content using the href attribute of the 'a' element\n",
    "                link = step.find('a')\n",
    "\n",
    "                if link:\n",
    "                    link_url = link.get('href', '')\n",
    "\n",
    "                    if link_url:\n",
    "                        content = application_steps_section.find(id=link_url.lstrip('#'))\n",
    "\n",
    "                        if content:\n",
    "                            content_text = content.find('p').text.strip()\n",
    "                            subheading_content[step_title] = content_text\n",
    "\n",
    "        # Find all 'a' elements within the provided HTML\n",
    "        a_links = soup.find_all('a')\n",
    "\n",
    "        # Iterate through the 'a' elements and write their href and text to the file\n",
    "        for a in a_links:\n",
    "            link_text = a.get_text(strip=True)\n",
    "            link_url = a.get('href', '')\n",
    "\n",
    "            if link_url:\n",
    "                # If the link has an href, check if it matches a subheading\n",
    "                if link_text in subheading_content:\n",
    "                    file.write(f\"{link_text} -> {subheading_content[link_text]}\\n\")\n",
    "                else:\n",
    "                    # If it doesn't match a subheading, write the link URL\n",
    "                    file.write(f\"{link_text} -> {link_url}\\n\")\n",
    "            else:\n",
    "                # If the link doesn't have an href, set it as a subheading\n",
    "                subheading = link_text\n",
    "        \n",
    "       \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter the URL: \")\n",
    "    extract_and_write_info_to_file(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
